# -*- coding: utf-8 -*-
"""DigitRecognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t-iobpGSIMkHvBRCYjP3OIY3zFBvEVSL

# **Installion and Setup**
"""

!pip install tensorflow

import tensorflow as tf

print(tf.__version__)

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

"""# **Data Preprocessing**"""

#importing the dataset
from tensorflow.keras.datasets import mnist

# loading the dataset
(x_train, y_train), (x_test, y_test) = mnist.load_data()

plt.imshow(x_train[1])

y_train[1]

x_train.shape, y_train.shape

x_test.shape, y_test.shape

x_train.min(), x_train.max()

y_train.min(), y_train.max()

#normalize the images
x_train = x_train/255.0
x_test = x_test/255.0

x_train.min(), x_train.max()

x_train.shape, x_test.shape

# reshape the images
x_train = x_train.reshape(60000, 28, 28, 1)
x_test = x_test.reshape(10000, 28, 28, 1)

x_train.shape, x_test.shape

input_shape = x_train[0].shape

print(input_shape)

"""# **Building the model**"""

# define an object
model = tf.keras.models.Sequential()

# Adding first CNN layer
model.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', input_shape = (28,28,1)))

# Adding second CNN layer
model.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu'))

# Adding maxpool layer
model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))

# Adding dropout layer
model.add(tf.keras.layers.Dropout(0.4))

# Adding flatten layer
model.add(tf.keras.layers.Flatten())

# dense layer
model.add(tf.keras.layers.Dense(units=128, activation='relu'))

#output layer
model.add(tf.keras.layers.Dense(units=10, activation='softmax'))

model.summary()

model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['sparse_categorical_accuracy'])

"""# **Training the model**"""

history = model.fit(x_train, y_train, batch_size=128, epochs=10, validation_data=(x_test, y_test))

# model predictions
# y_pred = model.predict_classes(x_test) old

# Assuming 'model' is your Sequential model
y_pred_probabilities = model.predict(x_test)
y_pred_classes = y_pred_probabilities.argmax(axis=-1)

# y_pred[3], y_test[3]
y_pred_classes[15], y_test[15]

# confusion matrix
from sklearn.metrics import confusion_matrix, accuracy_score

cm = confusion_matrix(y_pred_classes, y_test)
cm

acc_cm = accuracy_score(y_test, y_pred_classes)
acc_cm

"""# **Learning Curve**"""

def learning_curve(history, epoch):

  # training vs validation accuracy
  epoch_range = range(1, epoch+1)
  plt.plot(epoch_range, history.history['sparse_categorical_accuracy'])
  plt.plot(epoch_range, history.history['val_sparse_categorical_accuracy'])
  plt.title('Model Accuracy')
  plt.ylabel('Accuracy')
  plt.xlabel('Epoch')
  plt.legend(['Train','val'], loc='upper left')
  plt.show()

  # training vs validation loss
  plt.plot(epoch_range, history.history['loss'])
  plt.plot(epoch_range, history.history['val_loss'])
  plt.title('Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['Train','val'], loc='upper left')
  plt.show()

learning_curve(history, 10)

import tensorflow as tf
from tensorflow.keras.callbacks import ModelCheckpoint
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.datasets import mnist
from sklearn.metrics import confusion_matrix, accuracy_score

# ... (Your existing code)

# Mount Google Drive
from google.colab import drive
drive.mount('/content/gdrive')

# Define a ModelCheckpoint callback to save the best model during training
checkpoint_filepath = "/content/gdrive/MyDrive/best_model"
model_checkpoint_callback = ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=False,
    monitor='val_sparse_categorical_accuracy',  # You can choose the metric you want to monitor
    mode='max',
    save_best_only=True)

# Train the model with the ModelCheckpoint callback
history = model.fit(
    x_train, y_train,
    batch_size=128, epochs=10,
    validation_data=(x_test, y_test),
    callbacks=[model_checkpoint_callback])

# ... (Rest of your existing code)

# Learning curve function
def learning_curve():
    # Training vs validation accuracy
    epoch_range = range(1, 11)
    plt.plot(epoch_range, history.history['sparse_categorical_accuracy'])
    plt.plot(epoch_range, history.history['val_sparse_categorical_accuracy'])
    plt.title('Model Accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()

    # Training vs validation loss
    plt.plot(epoch_range, history.history['loss'])
    plt.plot(epoch_range, history.history['val_loss'])
    plt.title('Model Loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Validation'], loc='upper left')
    plt.show()

learning_curve()